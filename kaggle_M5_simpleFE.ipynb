{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 - Simple FEのまとめ\n",
    "https://www.kaggle.com/kyakovlev/m5-simple-fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#メモリの残量の表示\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "#データサイズの表示      \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ量を抑えることのできる関数\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2つのデータをmergeさせる関数\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使い回す変数の指定\n",
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1941         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "print('Load Main Data')\n",
    "\n",
    "# データの読み込み\n",
    "train_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv('sell_prices.csv')\n",
    "calendar_df = pd.read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python melt  \n",
    "http://nekoyukimmm.hatenablog.com/entry/2015/04/02/115045  \n",
    "２つ以上の値を格納している列columnを合体して、 １つの値valuesを格納している列columnを作成し、さらに、それに対応した認識子identifier variable(すなわち、もとの列名headerを)格納した列columnを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 59181090\n",
      "    Original grid_df:   3.6GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "print('Create Grid')\n",
    "\n",
    "# d_を1つの列にまとめる。'sales'も対応をさせる\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "# 出力した結果をいれるためのdataframeを作成\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# 一時的に作成したデータの削除\n",
    "del temp_df, add_grid,train_df\n",
    "\n",
    "# メモリの使用量の確認\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "#文字列のデータをカテゴリデータに変換をしている\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# カテゴリデータに変換をした上で再度メモリの使用量を確認\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   1.8GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "#商品ごとにいつ発売されてからの経過時間を調べる。\n",
    "print('Release week')\n",
    "\n",
    "#もし発売されていないなら削除することができるのでメモリの節約にもなる\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Now we can merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# 発売をされていないデータを削除したいので、カレンダーデータの一部とつきあわせる\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows and safe memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# データの使用量を計算\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# 'release'のデータ量を減らすために0からの表示変更をする\n",
    "#もともとはいつ発売されたかのdateが値として入っていた。\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# データの使用量を再度計算する\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (47735397, 10)\n"
     ]
    }
   ],
   "source": [
    "#いったんデータを保存する\n",
    "print('Save Part 1')\n",
    "#csvと比べて、pickleデータで保存をする方がメモリの節約になる\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "#料金データについて特徴量を作成していく\n",
    "print('Prices')\n",
    "\n",
    "# 基本統計量の算出を行う\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# max scalingを行う→どういった効果があるのかは謎\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# インフレ傾向にある商品もいくつかあるので価格の種類の多さも特徴量に加える\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# 月、年で平均値をとり変化率を調べる\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge prices and save part 2\n",
      "Mem. usage decreased to 1822.44 Mb (62.2% reduction)\n",
      "Size: (47735397, 13)\n"
     ]
    }
   ],
   "source": [
    "#料金データを加えたところで再度データの保存\n",
    "print('Merge prices and save part 2')\n",
    "\n",
    "# 上記で作成した料金データをmergeする\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# 2回目のデータの保存\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "del prices_df\n",
    "\n",
    "# We can remove new columns\n",
    "# or just load part_1\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 時系列のデータ処理\n",
    "https://qiita.com/motoki1990/items/8275dbe02d5fd5fa6d2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カレンダーデータを処理をしていく\n",
    "#MAIN_INDEX = ['id','d']\n",
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# 文字列から時系列データに変更を行う\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# dateをもとに基本的な時系列に関する特徴量を作成をしていく\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (47735397, 16)\n"
     ]
    }
   ],
   "source": [
    "#カレンダーデータを結合させる\n",
    "print('Save part 3')\n",
    "\n",
    "# Safe part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Some additional cleaning\n",
    "#################################################################################\n",
    "\n",
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk'\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Full Grid:   2.5GiB\n",
      "Size: (47735397, 34)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 34 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 2.5 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#SimpleFEでやってきたことのまとめ\n",
    "\n",
    "# 3つのデータをつなげて出力をしてみる\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "                     \n",
    "# メモリの使用量の確認\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "print('Size:', grid_df.shape)\n",
    "print(grid_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lags features→lags_df_28.pklの作成\n",
    "https://www.kaggle.com/kyakovlev/m5-lags-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時系列データを扱う際は必ず使えるものなのでコピペをしてうまく使うべき！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .assignによる特徴量の追加\n",
    "https://note.nkmk.me/python-pandas-assign-append/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lags\n",
      "8.52 min: Lags\n",
      "Create rolling aggs\n",
      "Rolling period: 7\n",
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 180\n",
      "Shifting period: 1\n",
      "Shifting period: 7\n",
      "Shifting period: 14\n",
      "17.52 min: Lags\n"
     ]
    }
   ],
   "source": [
    "#simpleFEを元に特徴量を作成する\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "# We need only 'id','d','sales'\n",
    "# to make lags and rollings\n",
    "grid_df = grid_df[['id','d','sales']]\n",
    "SHIFT_DAY = 28\n",
    "# 変数を作るのにかかった時間の計測\n",
    "#(time.time() - start_time) / 60)で出力できる\n",
    "start_time = time.time()\n",
    "\n",
    "#lag特徴量の作成を行う\n",
    "print('Create lags')\n",
    "#lagしたい日を内包表記で表す\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "# lag特徴量のメモリ数を制限する\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lag特徴量を作成したうえで移動平均を算出する(shiftさせる値は固定)\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "for i in [7,14,30,60,180]:\n",
    "    print('Rolling period:', i)\n",
    "    #移動平均と移動平均の標準偏差を求める\n",
    "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
    "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "# lag特徴量を作成したうえで移動平均を算出する(shiftさせる値もfor文で変えていく)\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,30,60]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "    \n",
    "    \n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save lags and rollings\n"
     ]
    }
   ],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "print('Save lags and rollings')\n",
    "grid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 40 columns):\n",
      "id                        category\n",
      "d                         int16\n",
      "sales                     float64\n",
      "sales_lag_28              float16\n",
      "sales_lag_29              float16\n",
      "sales_lag_30              float16\n",
      "sales_lag_31              float16\n",
      "sales_lag_32              float16\n",
      "sales_lag_33              float16\n",
      "sales_lag_34              float16\n",
      "sales_lag_35              float16\n",
      "sales_lag_36              float16\n",
      "sales_lag_37              float16\n",
      "sales_lag_38              float16\n",
      "sales_lag_39              float16\n",
      "sales_lag_40              float16\n",
      "sales_lag_41              float16\n",
      "sales_lag_42              float16\n",
      "rolling_mean_7            float16\n",
      "rolling_std_7             float16\n",
      "rolling_mean_14           float16\n",
      "rolling_std_14            float16\n",
      "rolling_mean_30           float16\n",
      "rolling_std_30            float16\n",
      "rolling_mean_60           float16\n",
      "rolling_std_60            float16\n",
      "rolling_mean_180          float16\n",
      "rolling_std_180           float16\n",
      "rolling_mean_tmp_1_7      float16\n",
      "rolling_mean_tmp_1_14     float16\n",
      "rolling_mean_tmp_1_30     float16\n",
      "rolling_mean_tmp_1_60     float16\n",
      "rolling_mean_tmp_7_7      float16\n",
      "rolling_mean_tmp_7_14     float16\n",
      "rolling_mean_tmp_7_30     float16\n",
      "rolling_mean_tmp_7_60     float16\n",
      "rolling_mean_tmp_14_7     float16\n",
      "rolling_mean_tmp_14_14    float16\n",
      "rolling_mean_tmp_14_30    float16\n",
      "rolling_mean_tmp_14_60    float16\n",
      "dtypes: category(1), float16(37), float64(1), int16(1)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features→mean_encoding_df.pkl\n",
    "https://www.kaggle.com/kyakovlev/m5-custom-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このカーネルでは、以下を示します。  \n",
    "  1.FE作成アプローチ  \n",
    "  2.順次fe検証  \n",
    "  3.次元削減  \n",
    "  4.重要度による特徴量検証  \n",
    "  5.targetエンコーディング  \n",
    "  6.作成した特徴量の並列化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.3\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(mem.percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple FEにて作成をしたもの\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3002725 entries, 0 to 3002724\n",
      "Data columns (total 34 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 161.9 MB\n"
     ]
    }
   ],
   "source": [
    "# すべてのデータがいらないのでとりあえず5%だけサンプリングを行う\n",
    "#idを行列表記にしてidの数()を20個に分割する\n",
    "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
    "#いったん上で分割した一つにフォーカスをしたいので、dataframeを作成する\n",
    "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
    "\n",
    "# Let's \"inspect\" our grid DataFrame\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's rmse: 2.85998\tvalid_1's rmse: 2.39811\n"
     ]
    }
   ],
   "source": [
    "#ベースモデルの作成\n",
    "\n",
    "# 今後、グローバルVARが必要になります\n",
    "\n",
    "SEED = 42             # すべてのランダムシード\n",
    "random.seed(SEED)     # すべてのテストを「確定的」にする\n",
    "np.random.seed(SEED)\n",
    "#N_CORESでcpuのコア数を表示することができる\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "TARGET = 'sales'      # Our Target\n",
    "END_TRAIN = 1941      # And we will use last 28 days as validation\n",
    "\n",
    "#simple feでtestデータ用に1941以降のデータも入っているが、ここでは絞って分析を行う \n",
    "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
    "\n",
    "# submissionのときに使うデータをまとめたもの\n",
    "remove_features = ['id','d',TARGET]\n",
    "\n",
    "# 私たちのベースラインモデルは、新機能のパフォーマンスを迅速にチェックするのに役立ちます\n",
    "# lightgbmのパラメーターを用意する\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
    "                    'objective': 'regression',       # Standart loss for RMSE\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'subsample': 0.8,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'early_stopping_rounds': 30,     # モデルのトレーニングはすぐに取りやめる \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "## 最小二乗法を考える\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "#高速機能テストを行うための小さな関数\n",
    "#estimator = make_fast_test（grid_df）\n",
    "#将来の分析のためにlgb boosterを返す\n",
    "\n",
    "#lightgbmで使うdataframeを用意する関数\n",
    "def make_fast_test(df):\n",
    "    #remove_featuresは['id','d',TARGET]→lightgbmの変数として使えないので分けていると思う\n",
    "    #features_columnsはlightgbmに使われる特徴量を集めたものである\n",
    "    features_columns = [col for col in list(df) if col not in remove_features]\n",
    "\n",
    "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]   \n",
    "    #訓練データの中の最後の28日間を今分析に用いる。分析結果として出力させたいやつではないってこと\n",
    "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
    "    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
    "    #実際にlightgbmで分析を行う\n",
    "    estimator = lgb.train(\n",
    "                            lgb_params,\n",
    "                            train_data,\n",
    "                            valid_sets = [train_data,valid_data],\n",
    "                            verbose_eval = 500,\n",
    "                        )\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "# Make baseline model\n",
    "baseline_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### この操作は結局lag1~7までの変数を作成をしているだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[378]\ttraining's rmse: 2.5463\tvalid_1's rmse: 2.25816\n"
     ]
    }
   ],
   "source": [
    "#通常のラグをテストしてみましょう（7日間）\n",
    "\n",
    "# poolは並列処理を可能にするライブラリらしいぞ\n",
    "from multiprocessing import Pool                \n",
    "\n",
    "##マルチプロセッシングの実行。\n",
    "#：t_split-ラグ日数のint ＃タイプ：int\n",
    "#：func-各分割に適用する関数：python関数\n",
    "##マルチプロセス実行→次の関数の下で実行されている\n",
    "def df_parallelize_run(func, t_split):\n",
    "    #N_CORESでcpuのコア数を表示することができる\n",
    "    #cpuを倉庫だとするとコア数っていうのは受け取った資材を処理する装置。これが多いと複数のデータを並列処理することが可能である\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "#売り上げに対してのlagの特徴量を作る関数\n",
    "def make_normal_lag(lag_day):\n",
    "    lag_df = grid_df[['id','d',TARGET]] \n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "# Launch parallel lag creation\n",
    "# and \"append\" to our grid\n",
    "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
    "#1週間分のlag特徴量を作成して結合する\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "\n",
    "# lightgbmで使うdataframeを用意する関数\n",
    "test_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここに詳細が書いてあるのでこれを確認するのが\n",
    "https://www.kaggle.com/dansbecker/permutation-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart RMSE 2.2581550604159593\n",
      "release 0.0\n",
      "sell_price 0.0047\n",
      "price_max 0.0008\n",
      "price_min 0.0003\n",
      "price_std 0.0029\n",
      "price_mean 0.0017\n",
      "price_norm 0.0135\n",
      "price_nunique 0.0003\n",
      "item_nunique -0.0002\n",
      "price_momentum 0.0001\n",
      "price_momentum_m 0.0048\n",
      "price_momentum_y 0.0015\n",
      "tm_d 0.0097\n",
      "tm_w 0.0003\n",
      "tm_m 0.0002\n",
      "tm_y 0.0\n",
      "tm_wm 0.0001\n",
      "tm_dw 0.1387\n",
      "tm_w_end 0.0099\n",
      "sales_lag_1 0.5717\n",
      "sales_lag_2 0.0461\n",
      "sales_lag_3 0.0213\n",
      "sales_lag_4 0.0153\n",
      "sales_lag_5 0.0201\n",
      "sales_lag_6 0.019\n",
      "sales_lag_7 0.0363\n"
     ]
    }
   ],
   "source": [
    "########################### 順列重要度テスト　###########################\n",
    "\n",
    "# features_columnsはlightgbmに使われる特徴量を集めたものである。remove_featuresは['id','d',TARGET]なので\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "#さっきlightgbmのところでも作成したvalidationのdataframe\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "\n",
    "# 予想が出来たデータを保存する\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "#誤差が一番小さくなるときを求める\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "\n",
    "# 分析の説明変数として使われたものをまとめてfor文で出力する\n",
    "for col in features_columns:\n",
    "    \n",
    "    # 検証セットのコピーを作成して復元します\n",
    "    # 各実行の状態を特徴とする\n",
    "    temp_df = validation_df.copy()\n",
    "    \n",
    "    # 「カテゴリ」機能があり、カテゴリを中断せずにnp.random.permutationを実行できない場合、ここでエラーが表示されます\n",
    "    #  したがって、特徴が数値であるかどうかを確認する必要があります\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        #random.permutationで配列をランダムに並び替え\n",
    "        #→正常のときとランダムにしたときの目的変数との相関関係を調べることで説明変数の重要度が測れるってこと\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        \n",
    "        #現在のrmseスコアがベーススコアより小さい場合それはおそらくその機能が悪いものであることを意味します\n",
    "        #私たちのモデルはノイズについて学習しています\n",
    "        #小数点以下4桁まで表示する\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# 要らないデータの削除\n",
    "del temp_df, validation_df\n",
    "\n",
    "# Remove test features\n",
    "#lagに関する特徴量だけを削除する対象とする\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# 結果\n",
    "## shift1はめっちゃ大事な変数である\n",
    "## 他のいくつかの機能は重要ではなく、おそらくノイズである\n",
    "## 機能の役に立たないことを確認するためにいくつかの順列実行を行う方が良い\n",
    "## price_nunique -0.002 : 強い負の値はおそらくノイズです\n",
    "## price_max -0.0002 : 0に近い値はより深い調査が必要です\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アイデアは次のとおりです。機能の重要性は、機能が利用できない場合にスコア（正確さ、mse、rmse、maeなど-興味のあるスコア）がどれだけ減少するかを調べることで測定できます。\n",
    "\n",
    "そのためには、データセットから特徴を削除し、推定器を再トレーニングしてスコアを確認します。  \n",
    "ただし、機能ごとに推定器を再トレーニングする必要があるため、計算量が多くなる可能性があります。  \n",
    "また、トレーニング済みの具体的なモデルでは何が重要であるかではなく、データセット内で何が重要かを示します。\n",
    "\n",
    "推定器の再トレーニングを回避するために、データセットのテスト部分からのみ機能を削除し、この機能を使用せずにスコアを計算できます。  \n",
    "推定者は機能が存在することを期待しているため、現状では機能しません。  \n",
    "したがって、フィーチャを削除する代わりに、**ランダムノイズで置き換える**ことができます。  \n",
    "フィーチャ列はまだ残っていますが、有用な情報は含まれていません。\n",
    "\n",
    "この方法は、**元の特徴値と同じ分布**からノイズが抽出された場合に機能します（そうしないと、推定器が失敗する可能性があります）。  \n",
    "このようなノイズを取得する最も簡単な方法は、特徴の値をシャッフルすることです。  \n",
    "つまり、他の例の特徴値を使用します。これが順列の重要度を計算する方法です。\n",
    "\n",
    "---\n",
    "\n",
    "# 機能が削除された場合（ノイズに置き換えられた場合）は適切ではありませんが、スコアは高くなります。シンプルで簡単。\n",
    "lagshiftの特徴量が重要なものであるとわかったので試しに2ヶ月後をやってみましたっていうこと  \n",
    "→lagの日数を変えた以外はほとんどやっていることは同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 2.86081\tvalid_1's rmse: 2.4008\n",
      "Standart RMSE 2.400802140889982\n",
      "release 0.0\n",
      "sell_price 0.015\n",
      "price_max 0.0068\n",
      "price_min 0.0056\n",
      "price_std 0.0069\n",
      "price_mean 0.009\n",
      "price_norm 0.0589\n",
      "price_nunique 0.0266\n",
      "item_nunique 0.0077\n",
      "price_momentum 0.0001\n",
      "price_momentum_m 0.0361\n",
      "price_momentum_y 0.0084\n",
      "tm_d 0.0055\n",
      "tm_w 0.0021\n",
      "tm_m 0.0\n",
      "tm_y 0.0\n",
      "tm_wm -0.0001\n",
      "tm_dw 0.1152\n",
      "tm_w_end 0.0113\n",
      "sales_lag_56 0.0325\n",
      "sales_lag_57 0.0097\n",
      "sales_lag_58 0.0042\n",
      "sales_lag_59 0.0032\n",
      "sales_lag_60 0.0009\n",
      "sales_lag_61 0.0048\n",
      "sales_lag_62 0.0066\n"
     ]
    }
   ],
   "source": [
    "########################### Lets test far away Lags (7 days with 56 days shift)　###########################\n",
    "########################### and check permutation importance　###########################\n",
    "\n",
    "#　2ヶ月後のデータも付け加えている\n",
    "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
    "#make_normal_lagは売り上げに対してのlagの特徴量を作る関数\n",
    "#df_parallelize_runは並列処理が出来る関数\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "#これ何度目だよ→特徴量にlagの56日分を加えてたので再度分析をしてるって訳だと思う\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "#これも上でやっているのと同じ処理\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "del temp_df, validation_df\n",
    "        \n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "# 56日のシフトがあるラグ（過去の遠い）はそれほど重要ではなく、最も近い過去のラグほど重要ではありません。\n",
    "# ある時点では、モデルにとって単なるノイズになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: id 7\n",
      "[0.72224136 0.06621842 0.05938444 0.04201445 0.03891686 0.03614344\n",
      " 0.03508102]\n",
      "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[292]\ttraining's rmse: 2.64421\tvalid_1's rmse: 2.26395\n"
     ]
    }
   ],
   "source": [
    "########################### PCA ###########################\n",
    "\n",
    "# PCA は教師なし学習\n",
    "# ターゲットがシフトしているので、ターゲットリークがないことを確認できます\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#後々pca_colには'id'が入る。n_days=7でこの分だけsalesのlag特徴量が作成される\n",
    "def make_pca(df, pca_col, n_days):\n",
    "    print('PCA:', pca_col, n_days)\n",
    "    \n",
    "    # 主成分分析には使用しない変数をまとめておく\n",
    "    pca_df = df[[pca_col,'d',TARGET]]\n",
    "    \n",
    "    # 変数を絞っていく作業\n",
    "    if pca_col != 'id':\n",
    "        merge_base = pca_df[[pca_col,'d']]\n",
    "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
    "        pca_df[TARGET] = pca_df['sum']\n",
    "        del pca_df['sum']\n",
    "    \n",
    "    #最大値によるスケーリングを行う\n",
    "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
    "    \n",
    "    # n_daysは関数で指定をしている\n",
    "    #今回は下のほうで7と指定をしているのでlag1~7まで作成をされる\n",
    "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
    "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
    "    #assignによって作成した特徴量を結合できる\n",
    "    pca_df = pca_df.assign(**{\n",
    "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
    "            for l in LAG_DAYS\n",
    "            for col in [TARGET]\n",
    "        })\n",
    "    #'id'と'd'は使わないので削除してしまうってことなはず\n",
    "    pca_columns = list(pca_df)[3:]\n",
    "    #欠損値を0で穴埋めする→PCAは欠損地があるとできないため\n",
    "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
    "    #実際に主成分分析の設定を行う\n",
    "    pca = PCA(random_state=SEED)\n",
    "    \n",
    "    #ここで実際に変換を行う\n",
    "    pca.fit(pca_df[pca_columns])\n",
    "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # 主成分分析で3次元までは残すってことなはず\n",
    "    keep_cols = pca_columns[:3]\n",
    "    print('Columns to keep:', keep_cols)\n",
    "    return pca_df[keep_cols]\n",
    "\n",
    "\n",
    "# 主成分分析によって特徴量を絞る\n",
    "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
    "\n",
    "# make_fast_testでlightgbmで使うdataframeを用意して分析まで行う関数\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id\n",
      "Encoding cat_id\n",
      "Encoding dept_id\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[404]\ttraining's rmse: 2.77462\tvalid_1's rmse: 2.39129\n"
     ]
    }
   ],
   "source": [
    "########################### Mean/std target encoding ########################### \n",
    "\n",
    "# targetencodingをするのにこの３つを使うらしい\n",
    "icols = ['item_id','cat_id','dept_id']\n",
    "\n",
    "#地味にtarget encodingを標準偏差と平均値によって行っている\n",
    "# icols = ['item_id','cat_id','dept_id']\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    temp_df = grid_df[grid_df['d']<=(1941-28)] # target encodingをやる際にはデータがleakageしないように注意をしないといけない\n",
    "    #aggの中にtargetを入れて分析をするんだと！感心しました！店ごとに区切ることで直接的にleakageをしないようにしているってことだと思う。\n",
    "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
    "    joiner = '_'+col+'_encoding_'\n",
    "    #この行の扱い方がイマイチわからない。なにをどうstripしているのか？？\n",
    "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
    "    del temp_df\n",
    "\n",
    "# make_fast_testでlightgbmで使うdataframeを用意して分析まで行う関数\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[337]\ttraining's rmse: 2.67507\tvalid_1's rmse: 2.299\n"
     ]
    }
   ],
   "source": [
    "########################### Last non O sale　###########################\n",
    "\n",
    "def find_last_sale(df,n_day):\n",
    "    \n",
    "    # Limit initial df\n",
    "    ls_df = df[['id','d',TARGET]]\n",
    "    \n",
    "    # 不等式にすることでいったんtrueかfalseかで表し、intにすることで0,1表記にしている。\n",
    "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
    "    \n",
    "    #lagを作成することでleakageを防いでいるらしい\n",
    "    #rollingの中の(2000,1)がよくわからない。\n",
    "    #fillna(-1)という-1はありえない値なのでこれを入れることでうまくデータを制御しているらしいぞ\n",
    "    #売れたか売れていないかの0,1の合計で説明変数を取ろうとしている\n",
    "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
    "\n",
    "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
    "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
    "\n",
    "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
    "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
    "\n",
    "    return ls_df[['last_sale']]\n",
    "\n",
    "\n",
    "# Find last non zero\n",
    "# Need some \"dances\" to fit in memory limit with groupers\n",
    "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>(1941-28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "            ]\n",
    "#こんなになんパターンもencodingってできるんだという感想\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 24 columns):\n",
      "id                           category\n",
      "d                            int16\n",
      "enc_state_id_mean            float16\n",
      "enc_state_id_std             float16\n",
      "enc_store_id_mean            float16\n",
      "enc_store_id_std             float16\n",
      "enc_cat_id_mean              float16\n",
      "enc_cat_id_std               float16\n",
      "enc_dept_id_mean             float16\n",
      "enc_dept_id_std              float16\n",
      "enc_state_id_cat_id_mean     float16\n",
      "enc_state_id_cat_id_std      float16\n",
      "enc_state_id_dept_id_mean    float16\n",
      "enc_state_id_dept_id_std     float16\n",
      "enc_store_id_cat_id_mean     float16\n",
      "enc_store_id_cat_id_std      float16\n",
      "enc_store_id_dept_id_mean    float16\n",
      "enc_store_id_dept_id_std     float16\n",
      "enc_item_id_mean             float16\n",
      "enc_item_id_std              float16\n",
      "enc_item_id_state_id_mean    float16\n",
      "enc_item_id_state_id_std     float16\n",
      "enc_item_id_store_id_mean    float16\n",
      "enc_item_id_store_id_std     float16\n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
