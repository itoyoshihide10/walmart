{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(mem.percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'sales'         # Our main target\n",
    "#これを更新をした！\n",
    "END_TRAIN = 1941         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "print('Load Main Data')\n",
    "\n",
    "# ここのvalidationをevaluationに変更をする\n",
    "train_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "prices_df = pd.read_csv('sell_prices.csv')\n",
    "calendar_df = pd.read_csv('calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 59181090\n",
      "    Original grid_df:   3.6GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "print('Create Grid')\n",
    "\n",
    "# We can tranform horizontal representation \n",
    "# to vertical \"view\"\n",
    "# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n",
    "# and labels are 'd_' coulmns\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# If we look on train_df we se that \n",
    "# we don't have a lot of traning rows\n",
    "# but each day can provide more train data\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "# we need to add \"test set\" to our grid\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We will not need original train_df\n",
    "# anymore and can remove it\n",
    "del train_df\n",
    "\n",
    "# You don't have to use df = df construction\n",
    "# you can use inplace=True instead.\n",
    "# like this\n",
    "# grid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   1.8GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Product Release date\n",
    "#################################################################################\n",
    "print('Release week')\n",
    "\n",
    "# It seems that leadings zero values\n",
    "# in each train_df item row\n",
    "# are not real 0 sales but mean\n",
    "# absence for the item in the store\n",
    "# we can safe some memory by removing\n",
    "# such zeros\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Now we can merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# We want to remove some \"zeros\" rows\n",
    "# from grid_df \n",
    "# to do it we need wm_yr_wk column\n",
    "# let's merge partly calendar_df to have it\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows \n",
    "# and safe memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Should we keep release week \n",
    "# as one of the features?\n",
    "# Only good CV can give the answer.\n",
    "# Let's minify the release values.\n",
    "# Min transformation will not help here \n",
    "# as int16 -> Integer (-32768 to 32767)\n",
    "# and our grid_df['release'].max() serves for int16\n",
    "# but we have have an idea how to transform \n",
    "# other columns in case we will need it\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (47735397, 10)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 1\n",
    "#################################################################################\n",
    "print('Save Part 1')\n",
    "\n",
    "# We have our BASE grid ready\n",
    "# and can save it as pickle file\n",
    "# for future use (model training)\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "########################### Prices\n",
    "#################################################################################\n",
    "print('Prices')\n",
    "\n",
    "# We can do some basic aggregations\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge prices and save part 2\n",
      "Mem. usage decreased to 1822.44 Mb (62.2% reduction)\n",
      "Size: (47735397, 13)\n"
     ]
    }
   ],
   "source": [
    "########################### Merge prices and save part 2\n",
    "#################################################################################\n",
    "print('Merge prices and save part 2')\n",
    "\n",
    "# Merge Prices\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Safe part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need prices_df anymore\n",
    "del prices_df\n",
    "\n",
    "# We can remove new columns\n",
    "# or just load part_1\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Merge calendar\n",
    "#################################################################################\n",
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (47735397, 16)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 3 (Dates)\n",
    "#################################################################################\n",
    "print('Save part 3')\n",
    "\n",
    "# Safe part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Some additional cleaning\n",
    "#################################################################################\n",
    "\n",
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk'\n",
    "# as test values are not in train set\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Full Grid:   2.5GiB\n",
      "Size: (47735397, 34)\n",
      "           Full Grid:   1.1GiB\n",
      "           Full Grid: 297.7MiB\n"
     ]
    }
   ],
   "source": [
    "########################### Summary\n",
    "#################################################################################\n",
    "\n",
    "# Now we have 3 sets of features\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "                     \n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# 2.5GiB + is is still too big to train our model\n",
    "# (on kaggle with its memory limits)\n",
    "# and we don't have lag features yet\n",
    "# But what if we can train by state_id or shop_id?\n",
    "state_id = 'CA'\n",
    "grid_df = grid_df[grid_df['state_id']==state_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid:   1.2GiB\n",
    "\n",
    "store_id = 'CA_1'\n",
    "grid_df = grid_df[grid_df['store_id']==store_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid: 321.2MiB\n",
    "\n",
    "# Seems its good enough now\n",
    "# In other kernel we will talk about LAGS features\n",
    "# Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4873639 entries, 0 to 47707955\n",
      "Data columns (total 34 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 297.7 MB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lags features→lags_df_28.pklの作成\n",
    "## これを参考にして特徴量を作成をできていたら良かった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1941         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "########################### Load Data\n",
    "#################################################################################\n",
    "print('Load Main Data')\n",
    "\n",
    "# We will need only train dataset\n",
    "# to show lags concept\n",
    "train_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "\n",
    "# To make all calculations faster\n",
    "# we will limit dataset by 'CA' state\n",
    "train_df = train_df[train_df['state_id']=='CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (12196, 1947)\n"
     ]
    }
   ],
   "source": [
    "########################### Data Representation\n",
    "#################################################################################\n",
    "\n",
    "# Let's check our shape\n",
    "print('Shape', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_006</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_007</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_009</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_010</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "5  HOBBIES_1_006_CA_1_evaluation  HOBBIES_1_006  HOBBIES_1  HOBBIES     CA_1   \n",
       "6  HOBBIES_1_007_CA_1_evaluation  HOBBIES_1_007  HOBBIES_1  HOBBIES     CA_1   \n",
       "7  HOBBIES_1_008_CA_1_evaluation  HOBBIES_1_008  HOBBIES_1  HOBBIES     CA_1   \n",
       "8  HOBBIES_1_009_CA_1_evaluation  HOBBIES_1_009  HOBBIES_1  HOBBIES     CA_1   \n",
       "9  HOBBIES_1_010_CA_1_evaluation  HOBBIES_1_010  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "5       CA    0    0    0    0  ...       2       1       0       0       1   \n",
       "6       CA    0    0    0    0  ...       0       1       0       0       0   \n",
       "7       CA   12   15    0    0  ...       7       0       6       0      15   \n",
       "8       CA    2    0    7    3  ...       1       0       0       0       0   \n",
       "9       CA    0    0    1    0  ...       0       0       1       0       2   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "5       0       0       5       2       0  \n",
       "6       1       0       1       1       0  \n",
       "7       5       4       1      40      32  \n",
       "8       0       0       0       1       0  \n",
       "9       1       1       0       0       1  \n",
       "\n",
       "[10 rows x 1947 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Horizontal representation\n",
    "\n",
    "# If we feed directly this data to model\n",
    "# our label will be values in column 'd_1941'\n",
    "# all other columns will be our \"features\"\n",
    "\n",
    "# In lag terminology all d_1->d_1912 columns\n",
    "# are our lag features \n",
    "# (target values in previous time period)\n",
    "\n",
    "# Good thing that we have a lot of features here\n",
    "# Bad thing is that we have just 12196 \"train rows\"\n",
    "# Note: here and after all numbers are limited to 'CA' state\n",
    "train_df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36588</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48784</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60980</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73176</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85372</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97568</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109764</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d</td>\n",
       "      <td>d_10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id        item_id    dept_id   cat_id  \\\n",
       "0       HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "12196   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "24392   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "36588   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "48784   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "60980   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "73176   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "85372   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "97568   HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "109764  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "\n",
       "       store_id state_id  d sales  \n",
       "0          CA_1       CA  d   d_1  \n",
       "12196      CA_1       CA  d   d_2  \n",
       "24392      CA_1       CA  d   d_3  \n",
       "36588      CA_1       CA  d   d_4  \n",
       "48784      CA_1       CA  d   d_5  \n",
       "60980      CA_1       CA  d   d_6  \n",
       "73176      CA_1       CA  d   d_7  \n",
       "85372      CA_1       CA  d   d_8  \n",
       "97568      CA_1       CA  d   d_9  \n",
       "109764     CA_1       CA  d  d_10  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vertical representation\n",
    "\n",
    "# In other hand we can think of d_ columns\n",
    "# as additional labels and can significantly \n",
    "# scale up our training set to 23330948 rows\n",
    "\n",
    "# Good thing that our model will have \n",
    "# greater input for training\n",
    "# Bad thing that we are losing lags that we had\n",
    "# in horizontal representation and\n",
    "# also new data set consumes much more memory\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "train_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "train_df[train_df['id']=='HOBBIES_1_001_CA_1_evaluation'].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some minification\n",
    "train_df['d'] = train_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "icols = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "for col in icols:\n",
    "    train_df[col] = train_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifting: 1\n",
      "Shifting: 2\n",
      "Shifting: 3\n",
      "Shifting: 4\n",
      "Shifting: 5\n",
      "Shifting: 6\n",
      "Shifting: 7\n",
      "1.93 min: Time for loops\n",
      "1.89 min: Time for bulk shift\n"
     ]
    }
   ],
   "source": [
    "########################### Lags creation\n",
    "#################################################################################\n",
    "\n",
    "# We have several \"code\" solutions here\n",
    "# As our dataset is allready sorted by d values\n",
    "# we can simply shift() values\n",
    "# also we have to keep in mind that \n",
    "# we need to aggregate values on 'id' level\n",
    "\n",
    "# group and shift in loop\n",
    "temp_df = train_df[['id','d',TARGET]]\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(1,8):\n",
    "    print('Shifting:', i)\n",
    "    temp_df['lag_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(i))\n",
    "    \n",
    "print('%0.2f min: Time for loops' % ((time.time() - start_time) / 60))\n",
    "\n",
    "\n",
    "# Or same in \"compact\" manner\n",
    "LAG_DAYS = [col for col in range(1,8)]\n",
    "temp_df = train_df[['id','d',TARGET]]\n",
    "\n",
    "start_time = time.time()\n",
    "temp_df = temp_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): temp_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "print('%0.2f min: Time for bulk shift' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>sales_lag_1</th>\n",
       "      <th>sales_lag_2</th>\n",
       "      <th>sales_lag_3</th>\n",
       "      <th>sales_lag_4</th>\n",
       "      <th>sales_lag_5</th>\n",
       "      <th>sales_lag_6</th>\n",
       "      <th>sales_lag_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, d, sales, sales_lag_1, sales_lag_2, sales_lag_3, sales_lag_4, sales_lag_5, sales_lag_6, sales_lag_7]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result\n",
    "temp_df[temp_df['id']=='HOBBIES_1_001_CA_1_evaluation'].iloc[:10]\n",
    "\n",
    "# You can notice many NaNs values - it's normal\n",
    "# because there is no data for day 0,-1,-2\n",
    "# (out of dataset time periods)\n",
    "\n",
    "# Same works for test set\n",
    "# be careful to make lag features:\n",
    "# for day 1920 there is no data about day 1919 (until 1913)\n",
    "# So if you want to predict day 1915 your \n",
    "# lag features have to start from 2 \n",
    "# (1915(predicting day) - 1913(last day with label in dataset))\n",
    "# and so on.\n",
    "\n",
    "# There are few options to work \n",
    "# with NaNs in train set\n",
    "## 1. drop it train_df[train_df['d']>MAX_LAG_DAY] \n",
    "## 1.1 in our case we already dropped some lines by release date\n",
    "##     so you have find d.min() for each id\n",
    "##     and drop train_df[train_df['d']>(train_df['d_min']+MAX_LAG_DAY)] \n",
    "## 2. If you want to keep it you can \n",
    "##    fill with '-1' to be able to convert to int\n",
    "## 3. Leave as it is\n",
    "## 4. Fill with mean -> not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "2.29 min: Time for loop\n"
     ]
    }
   ],
   "source": [
    "########################### Rolling lags\n",
    "#################################################################################\n",
    "\n",
    "# We restored some day sales values from horizontal representation\n",
    "# as lag features but just few of them (last 7 days or less)\n",
    "# because of memory limits we can't have many lag features\n",
    "# How we can get additional information from other days?\n",
    "\n",
    "## Rolling aggragations\n",
    "\n",
    "temp_df = train_df[['id','d','sales']]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in [14,30,60]:\n",
    "    print('Rolling period:', i)\n",
    "    temp_df['rolling_mean_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n",
    "    temp_df['rolling_std_'+str(i)]  = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n",
    "\n",
    "# lambda x: x.shift(1)\n",
    "# 1 day shift will serve only to predict day 1914\n",
    "# for other days you have to shift PREDICT_DAY-1913\n",
    "\n",
    "# Such aggregations will help us to restore\n",
    "# at least part of the information for our model\n",
    "# and out of 14+30+60->104 columns we can have just 6\n",
    "# with valuable information (hope it is sufficient)\n",
    "# you can also aggregate by max/skew/median etc \n",
    "# also you can try other rolling periods 180,365 etc\n",
    "print('%0.2f min: Time for loop' % ((time.time() - start_time) / 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>rolling_mean_14</th>\n",
       "      <th>rolling_std_14</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>rolling_mean_60</th>\n",
       "      <th>rolling_std_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, d, sales, rolling_mean_14, rolling_std_14, rolling_mean_30, rolling_std_30, rolling_mean_60, rolling_std_60]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result\n",
    "temp_df[temp_df['id']=='HOBBIES_1_002_CA_1_evaluation'].iloc[:20]\n",
    "\n",
    "# Same for NaNs values - it's normal\n",
    "# because there is no data for \n",
    "# 0*(rolling_period),-1*(rolling_period),-2*(rolling_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original rolling df:   1.3GiB\n",
      "   Values rolling df:   1.1GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Memory ussage\n",
    "#################################################################################\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# can we minify it?\n",
    "# 1. if our dataset are aligned by index \n",
    "#    you don't need 'id' 'd' 'sales' columns\n",
    "temp_df = temp_df.iloc[:,3:]\n",
    "print(\"{:>20}: {:>8}\".format('Values rolling df',sizeof_fmt(temp_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# can we make it even smaller?\n",
    "# carefully change dtype and/or\n",
    "# use sparce matrix to minify 0s\n",
    "# Also note that lgbm accepts matrixes as input\n",
    "# that is good for memory reducion \n",
    "from scipy import sparse \n",
    "temp_matrix = sparse.csr_matrix(temp_df)\n",
    "\n",
    "# restore to df\n",
    "temp_matrix_restored = pd.DataFrame(temp_matrix.todense())\n",
    "restored_cols = ['roll_' + str(i) for i in list(temp_matrix_restored)]\n",
    "temp_matrix_restored.columns = restored_cols\n",
    "########################### Remove old objects\n",
    "#################################################################################\n",
    "del temp_df, train_df, temp_matrix, temp_matrix_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lags\n",
      "9.67 min: Lags\n",
      "Create rolling aggs\n",
      "Rolling period: 7\n",
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 180\n",
      "Shifting period: 1\n",
      "Shifting period: 7\n",
      "Shifting period: 14\n",
      "20.10 min: Lags\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "\n",
    "# We need only 'id','d','sales'\n",
    "# to make lags and rollings\n",
    "grid_df = grid_df[['id','d','sales']]\n",
    "SHIFT_DAY = 28\n",
    "\n",
    "# Lags\n",
    "# with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create lags')\n",
    "\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
    "grid_df = grid_df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "# Minify lag columns\n",
    "for col in list(grid_df):\n",
    "    if 'lag' in col:\n",
    "        grid_df[col] = grid_df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "# Rollings\n",
    "# with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "\n",
    "for i in [7,14,30,60,180]:\n",
    "    print('Rolling period:', i)\n",
    "    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
    "    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "# Rollings\n",
    "# with sliding shift\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,30,60]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "    \n",
    "    \n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save lags and rollings\n"
     ]
    }
   ],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "print('Save lags and rollings')\n",
    "grid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 40 columns):\n",
      "id                        category\n",
      "d                         int16\n",
      "sales                     float64\n",
      "sales_lag_28              float16\n",
      "sales_lag_29              float16\n",
      "sales_lag_30              float16\n",
      "sales_lag_31              float16\n",
      "sales_lag_32              float16\n",
      "sales_lag_33              float16\n",
      "sales_lag_34              float16\n",
      "sales_lag_35              float16\n",
      "sales_lag_36              float16\n",
      "sales_lag_37              float16\n",
      "sales_lag_38              float16\n",
      "sales_lag_39              float16\n",
      "sales_lag_40              float16\n",
      "sales_lag_41              float16\n",
      "sales_lag_42              float16\n",
      "rolling_mean_7            float16\n",
      "rolling_std_7             float16\n",
      "rolling_mean_14           float16\n",
      "rolling_std_14            float16\n",
      "rolling_mean_30           float16\n",
      "rolling_std_30            float16\n",
      "rolling_mean_60           float16\n",
      "rolling_std_60            float16\n",
      "rolling_mean_180          float16\n",
      "rolling_std_180           float16\n",
      "rolling_mean_tmp_1_7      float16\n",
      "rolling_mean_tmp_1_14     float16\n",
      "rolling_mean_tmp_1_30     float16\n",
      "rolling_mean_tmp_1_60     float16\n",
      "rolling_mean_tmp_7_7      float16\n",
      "rolling_mean_tmp_7_14     float16\n",
      "rolling_mean_tmp_7_30     float16\n",
      "rolling_mean_tmp_7_60     float16\n",
      "rolling_mean_tmp_14_7     float16\n",
      "rolling_mean_tmp_14_14    float16\n",
      "rolling_mean_tmp_14_30    float16\n",
      "rolling_mean_tmp_14_60    float16\n",
      "dtypes: category(1), float16(37), float64(1), int16(1)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom features→mean_encoding_df.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの名前の変更と最終日の変更を必ず行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this kernel I would like to show: \n",
    " 1. FE creation approaches  \n",
    " 2. Sequential fe validation  \n",
    " 3. Dimension reduction  \n",
    " 4. FE validation by Permutation importance  \n",
    " 5. Mean encodings  \n",
    " 6. Parallelization for FE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(mem.percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple FEにて作成をしたもの\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
    "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3002725 entries, 0 to 3002724\n",
      "Data columns (total 34 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 161.9 MB\n"
     ]
    }
   ],
   "source": [
    "########################### Load data\n",
    "########################### Basic features were created here:\n",
    "########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "# Subsampling\n",
    "# to make all calculations faster.\n",
    "# Keep only 5% of original ids.\n",
    "#idを行列表記にしてidの数()を20個に分割する\n",
    "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
    "#いったん上で分割した一つにフォーカスをしたいので、dataframeを作成する\n",
    "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
    "\n",
    "# Let's \"inspect\" our grid DataFrame\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[379]\ttraining's rmse: 2.79812\tvalid_1's rmse: 2.39787\n"
     ]
    }
   ],
   "source": [
    "########################### Baseline model\n",
    "#################################################################################\n",
    "\n",
    "# 今後、グローバルVARが必要になります\n",
    "\n",
    "SEED = 42             # すべてのランダムシード\n",
    "random.seed(SEED)     # すべてのテストを「確定的」にする\n",
    "np.random.seed(SEED)\n",
    "#N_CORESでcpuのコア数を表示することができる\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "TARGET = 'sales'      # Our Target\n",
    "END_TRAIN = 1941      # And we will use last 28 days as validation\n",
    "\n",
    "#simple feでtestデータ用に1914以降のデータも入っているが、ここでは絞って分析を行う \n",
    "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
    "\n",
    "# Features that we want to exclude from training\n",
    "remove_features = ['id','d',TARGET]\n",
    "\n",
    "# 私たちのベースラインモデルは、新機能のパフォーマンスを迅速にチェックするのに役立ちます\n",
    "\n",
    "# lightgbmのパラメーターを用意する\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
    "                    'objective': 'regression',       # Standart loss for RMSE\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'subsample': 0.8,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "## 最小二乗法を考える\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "#高速機能テストを行うための小さな関数\n",
    "#estimator = make_fast_test（grid_df）\n",
    "#将来の分析のためにlgb boosterを返す\n",
    "\n",
    "#lightgbmで使うdataframeを用意する関数\n",
    "def make_fast_test(df):\n",
    "    #remove_featuresは['id','d',TARGET]→lightgbmの変数として使えないので分けていると思う\n",
    "    #features_columnsはlightgbmに使われる特徴量を集めたものである\n",
    "    features_columns = [col for col in list(df) if col not in remove_features]\n",
    "\n",
    "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]   \n",
    "    #訓練データの中の最後の28日間を今分析に用いる。分析結果として出力させたいやつではないってこと\n",
    "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
    "    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                            lgb_params,\n",
    "                            train_data,\n",
    "                            valid_sets = [train_data,valid_data],\n",
    "                            verbose_eval = 500,\n",
    "                        )\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "# Make baseline model\n",
    "baseline_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これが今回の結果  \n",
    "Training until validation scores don't improve for 30 rounds   \n",
    "Early stopping, best iteration is:  \n",
    "[379]\ttraining's rmse: 2.79812\tvalid_1's rmse: 2.39787  \n",
    "\n",
    "こっちが前回の結果  \n",
    "Training until validation scores don't improve for 30 rounds  \n",
    "Early stopping, best iteration is:  \n",
    "[318]\ttraining's rmse: 2.82659\tvalid_1's rmse: 2.38847  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### この操作が謎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\ttraining's rmse: 2.59276\tvalid_1's rmse: 2.26285\n"
     ]
    }
   ],
   "source": [
    "#通常のラグをテストしてみましょう（7日間）\n",
    "\n",
    "# poolは並列処理を可能にするライブラリらしいぞ\n",
    "from multiprocessing import Pool                \n",
    "\n",
    "##マルチプロセッシングの実行。\n",
    "#：t_split-ラグ日数のint ＃タイプ：int\n",
    "#：func-各分割に適用する関数：python関数\n",
    "##マルチプロセス実行→次の関数の下で実行されている\n",
    "def df_parallelize_run(func, t_split):\n",
    "    #N_CORESでcpuのコア数を表示することができる\n",
    "    #cpuを倉庫だとするとコア数っていうのは受け取った資材を処理する装置。これが多いと複数のデータを並列処理することが可能である\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "#売り上げに対してのlagの特徴量を作る関数\n",
    "def make_normal_lag(lag_day):\n",
    "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "# Launch parallel lag creation\n",
    "# and \"append\" to our grid\n",
    "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "\n",
    "# lightgbmで使うdataframeを用意する関数\n",
    "test_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここに詳細が書いてあるのでこれを確認するのが\n",
    "https://www.kaggle.com/dansbecker/permutation-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart RMSE 2.2628541497944274\n",
      "release 0.0\n",
      "sell_price 0.0038\n",
      "price_max 0.0006\n",
      "price_min 0.001\n",
      "price_std 0.0021\n",
      "price_mean 0.0022\n",
      "price_norm 0.0088\n",
      "price_nunique 0.0003\n",
      "item_nunique 0.0007\n",
      "price_momentum 0.0\n",
      "price_momentum_m 0.0046\n",
      "price_momentum_y 0.0003\n",
      "tm_d 0.0094\n",
      "tm_w -0.0001\n",
      "tm_m -0.0002\n",
      "tm_y 0.0\n",
      "tm_wm 0.0002\n",
      "tm_dw 0.1406\n",
      "tm_w_end 0.0086\n",
      "sales_lag_1 0.5844\n",
      "sales_lag_2 0.0418\n",
      "sales_lag_3 0.0189\n",
      "sales_lag_4 0.0143\n",
      "sales_lag_5 0.0184\n",
      "sales_lag_6 0.0214\n",
      "sales_lag_7 0.0453\n"
     ]
    }
   ],
   "source": [
    "########################### 順列重要度テスト　###########################\n",
    "\n",
    "# Let's creat validation dataset and features\n",
    "# features_columnsはlightgbmに使われる特徴量を集めたものである。remove_featuresは['id','d',TARGET]なので\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "#さっきlightgbmのところでも作成したvalidationのdataframe\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "\n",
    "# 予想が出来たデータを保存する\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "#誤差が一番小さくなると\n",
    "きを求める\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "\n",
    "# 分析の説明変数として使われたものをまとめてfor文で出力する\n",
    "for col in features_columns:\n",
    "    \n",
    "    # 検証セットのコピーを作成して復元します\n",
    "    # 各実行の状態を特徴とする\n",
    "    temp_df = validation_df.copy()\n",
    "    \n",
    "    # 「カテゴリ」機能があり、カテゴリを中断せずにnp.random.permutationを実行できない場合、ここでエラーが表示されます\n",
    "    #  したがって、特徴が数値であるかどうかを確認する必要があります\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        #random.permutationで配列をランダムに並び替え\n",
    "        #→正常のときとランダムにしたときの目的変数との相関関係を調べることで説明変数の重要度が測れるってこと\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        \n",
    "        #　現在のrmseスコアがベーススコアより小さい場合\n",
    "　　　　#　それはおそらくその機能が悪いものであることを意味します\n",
    "        #　私たちのモデルはノイズについて学習しています\n",
    "        #　小数点以下4桁まで表示する\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# 要らないデータの削除\n",
    "del temp_df, validation_df\n",
    "\n",
    "# Remove test features\n",
    "#lagに関する特徴量だけを削除する対象とする\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# 結果\n",
    "## shift1はめっちゃ大事な変数である\n",
    "## 他のいくつかの機能は重要ではなく、おそらくノイズだけです\n",
    "## 機能の役に立たないことを確認するためにいくつかの順列実行を行う方が良い\n",
    "## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
    "\n",
    "## price_nunique -0.002 : strong negative values are most probably noise\n",
    "## price_max -0.0002 : values close to 0 need deeper investigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アイデアは次のとおりです。機能の重要性は、機能が利用できない場合にスコア（正確さ、mse、rmse、maeなど-興味のあるスコア）がどれだけ減少するかを調べることで測定できます。\n",
    "\n",
    "そのためには、データセットから特徴を削除し、推定器を再トレーニングしてスコアを確認します。ただし、機能ごとに推定器を再トレーニングする必要があるため、計算量が多くなる可能性があります。また、トレーニング済みの具体的なモデルでは何が重要であるかではなく、データセット内で何が重要かを示します。\n",
    "\n",
    "推定器の再トレーニングを回避するために、データセットのテスト部分からのみ機能を削除し、この機能を使用せずにスコアを計算できます。推定者は機能が存在することを期待しているため、現状では機能しません。したがって、フィーチャを削除する代わりに、**ランダムノイズで置き換える**ことができます-フィーチャ列はまだ残っていますが、有用な情報は含まれていません。この方法は、**元の特徴値と同じ分布**からノイズが抽出された場合に機能します（そうしないと、推定器が失敗する可能性があります）。このようなノイズを取得する最も簡単な方法は、特徴の値をシャッフルすることです。つまり、他の例の特徴値を使用します。これが順列の重要度を計算する方法です。\n",
    "\n",
    "---\n",
    "\n",
    "# 機能が削除された場合（ノイズに置き換えられた場合）は適切ではありませんが、スコアは高くなります。シンプルで簡単。\n",
    "lagshiftの1日が重要なものであるとわかったので試しに2ヶ月後をやってみましたっていうこと  \n",
    "→lagの日数を変えた以外はほとんどやっていることは同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttraining's rmse: 2.84225\tvalid_1's rmse: 2.40565\n",
      "Standart RMSE 2.405646939965529\n",
      "release 0.0\n",
      "sell_price 0.0238\n",
      "price_max 0.0036\n",
      "price_min 0.0029\n",
      "price_std 0.009\n",
      "price_mean 0.0057\n",
      "price_norm 0.0486\n",
      "price_nunique 0.0179\n",
      "item_nunique 0.0061\n",
      "price_momentum -0.0003\n",
      "price_momentum_m 0.0351\n",
      "price_momentum_y 0.009\n",
      "tm_d 0.0054\n",
      "tm_w 0.0001\n",
      "tm_m -0.0004\n",
      "tm_y 0.0\n",
      "tm_wm -0.0\n",
      "tm_dw 0.118\n",
      "tm_w_end 0.0108\n",
      "sales_lag_56 0.0228\n",
      "sales_lag_57 0.0087\n",
      "sales_lag_58 0.0032\n",
      "sales_lag_59 0.0031\n",
      "sales_lag_60 -0.0016\n",
      "sales_lag_61 0.002\n",
      "sales_lag_62 0.0062\n"
     ]
    }
   ],
   "source": [
    "########################### Lets test far away Lags (7 days with 56 days shift)　###########################\n",
    "########################### and check permutation importance　###########################\n",
    "\n",
    "#　2ヶ月後のデータも付け加えている\n",
    "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
    "#make_normal_lagは売り上げに対してのlagの特徴量を作る関数\n",
    "#df_parallelize_runは並列処理が出来る関数\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "#これ何度目だよ→特徴量にlagの56日分を加えてたので再度分析をしてるって訳だと思う\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "del temp_df, validation_df\n",
    "        \n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "## Lags with 56 days shift (far away past) are not as important\n",
    "## as nearest past lags\n",
    "## and at some point will be just noise for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: id 7\n",
      "[0.72224136 0.06621842 0.05938444 0.04201445 0.03891686 0.03614344\n",
      " 0.03508102]\n",
      "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttraining's rmse: 2.58932\tvalid_1's rmse: 2.26147\n"
     ]
    }
   ],
   "source": [
    "########################### PCA ###########################\n",
    "\n",
    "# ここでの主な質問-私たちは持つことができます\n",
    "#機能が少なくてもほぼ同じrmseブースト\n",
    "#次元が少ない？\n",
    "\n",
    "# PCAを試して、7から3次元に削減してみましょう\n",
    "\n",
    "# PCA は教師なし学習\n",
    "# ターゲットがシフトしているので、ターゲットリークがないことを確認できます\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#後々pca_colには'id'が入る。n_days=7でこの分だけsalesのlag特徴量が作成される\n",
    "def make_pca(df, pca_col, n_days):\n",
    "    print('PCA:', pca_col, n_days)\n",
    "    \n",
    "    # We don't need any other columns to make pca\n",
    "    pca_df = df[[pca_col,'d',TARGET]]\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\" \n",
    "    # we need to agg first\n",
    "    if pca_col != 'id':\n",
    "        merge_base = pca_df[[pca_col,'d']]\n",
    "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
    "        pca_df[TARGET] = pca_df['sum']\n",
    "        del pca_df['sum']\n",
    "    \n",
    "    #最大値によるスケーリングを行う\n",
    "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
    "    \n",
    "    # てかさっきこれ作らなかったっけ？\n",
    "    #→おそらくさっき56日分のものを作った際に1回消してしまっているので再度つくるっていう流れのはず\n",
    "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
    "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
    "    pca_df = pca_df.assign(**{\n",
    "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
    "            for l in LAG_DAYS\n",
    "            for col in [TARGET]\n",
    "        })\n",
    "    #'id'と'd'は使わないので削除してしまうってことなはず\n",
    "    pca_columns = list(pca_df)[3:]\n",
    "    #欠損値を0で穴埋めする\n",
    "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
    "    #実際に主成分分析の設定を行う\n",
    "    pca = PCA(random_state=SEED)\n",
    "    \n",
    "    #ここで実際に変換を行う\n",
    "    pca.fit(pca_df[pca_columns])\n",
    "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
    "    \n",
    "    print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # 主成分分析で最低でも3次元までは残すってことなはず\n",
    "    keep_cols = pca_columns[:3]\n",
    "    print('Columns to keep:', keep_cols)\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\"\n",
    "    # we need merge back our results to merge_base df\n",
    "    # and only than return resulted df\n",
    "    # I'll skip that step here\n",
    "    \n",
    "    return pca_df[keep_cols]\n",
    "\n",
    "\n",
    "# Make PCA\n",
    "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id\n",
      "Encoding cat_id\n",
      "Encoding dept_id\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[400]\ttraining's rmse: 2.77636\tvalid_1's rmse: 2.39284\n"
     ]
    }
   ],
   "source": [
    "########################### Mean/std target encoding\n",
    "#################################################################################\n",
    "\n",
    "# targetencodingをするのにこの３つを使うらしい\n",
    "icols = ['item_id','cat_id','dept_id']\n",
    "\n",
    "# But we can use any other column or even multiple groups\n",
    "# like these ones\n",
    "#            'state_id',\n",
    "#            'store_id',\n",
    "#            'cat_id',\n",
    "#            'dept_id',\n",
    "#            ['state_id', 'cat_id'],\n",
    "#            ['state_id', 'dept_id'],\n",
    "#            ['store_id', 'cat_id'],\n",
    "#            ['store_id', 'dept_id'],\n",
    "#            'item_id',\n",
    "#            ['item_id', 'state_id'],\n",
    "#            ['item_id', 'store_id']\n",
    "\n",
    "# There are several ways to do \"mean\" encoding\n",
    "## K-fold scheme\n",
    "## LOO (leave one out)\n",
    "## Smoothed/regularized \n",
    "## Expanding mean\n",
    "## etc \n",
    "\n",
    "# You can test as many options as you want\n",
    "# and decide what to use\n",
    "# Because of memory issues you can't \n",
    "# use many features.\n",
    "\n",
    "#地味にtarget encodingを標準偏差と平均値によって行っている\n",
    "# icols = ['item_id','cat_id','dept_id']\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    temp_df = grid_df[grid_df['d']<=(1913-28)] # target encodingをやる際にはデータがleakageしないように注意をしないといけない\n",
    "    #aggの中にtargetを入れて分析をするんだと！感心しました！店ごとに区切ることで直接的にleakageをしないようにしているってことだと思う。\n",
    "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
    "    joiner = '_'+col+'_encoding_'\n",
    "    #この行の扱い方がイマイチわからない。なにをどうstripしているのか？？\n",
    "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
    "    del temp_df\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "# Bad thing that for some items  \n",
    "# we are using past and future values.\n",
    "# But we are looking for \"categorical\" similiarity\n",
    "# on a \"long run\". So future here is not a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[349]\ttraining's rmse: 2.6723\tvalid_1's rmse: 2.29713\n"
     ]
    }
   ],
   "source": [
    "########################### Last non O sale　###########################\n",
    "\n",
    "def find_last_sale(df,n_day):\n",
    "    \n",
    "    # Limit initial df\n",
    "    ls_df = df[['id','d',TARGET]]\n",
    "    \n",
    "    # 不等式にすることでいったんtrueかfalseかで表し、intにすることで0,1表記にしている。\n",
    "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
    "    \n",
    "    #lagを作成することでleakageを防いでいるらしい\n",
    "    #rollingの中の(2000,1)がよくわからない。\n",
    "    #fillna(-1)という-1はありえない値なのでこれを入れることでうまくデータを制御しているらしいぞ\n",
    "    #売れたか売れていないかの0,1の合計で説明変数を取ろうとしている\n",
    "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
    "\n",
    "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
    "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
    "\n",
    "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
    "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
    "\n",
    "    return ls_df[['last_sale']]\n",
    "\n",
    "\n",
    "# Find last non zero\n",
    "# Need some \"dances\" to fit in memory limit with groupers\n",
    "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>(1941-28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "            ]\n",
    "#こんなになんパターンもencodingってできるんだという感想\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47735397 entries, 0 to 47735396\n",
      "Data columns (total 24 columns):\n",
      "id                           category\n",
      "d                            int16\n",
      "enc_state_id_mean            float16\n",
      "enc_state_id_std             float16\n",
      "enc_store_id_mean            float16\n",
      "enc_store_id_std             float16\n",
      "enc_cat_id_mean              float16\n",
      "enc_cat_id_std               float16\n",
      "enc_dept_id_mean             float16\n",
      "enc_dept_id_std              float16\n",
      "enc_state_id_cat_id_mean     float16\n",
      "enc_state_id_cat_id_std      float16\n",
      "enc_state_id_dept_id_mean    float16\n",
      "enc_state_id_dept_id_std     float16\n",
      "enc_store_id_cat_id_mean     float16\n",
      "enc_store_id_cat_id_std      float16\n",
      "enc_store_id_dept_id_mean    float16\n",
      "enc_store_id_dept_id_std     float16\n",
      "enc_item_id_mean             float16\n",
      "enc_item_id_std              float16\n",
      "enc_item_id_state_id_mean    float16\n",
      "enc_item_id_state_id_std     float16\n",
      "enc_item_id_store_id_mean    float16\n",
      "enc_item_id_store_id_std     float16\n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf=pd.read_pickle('lags_df_28.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>sales_lag_28</th>\n",
       "      <th>sales_lag_29</th>\n",
       "      <th>sales_lag_30</th>\n",
       "      <th>sales_lag_31</th>\n",
       "      <th>sales_lag_32</th>\n",
       "      <th>sales_lag_33</th>\n",
       "      <th>sales_lag_34</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_mean_tmp_1_30</th>\n",
       "      <th>rolling_mean_tmp_1_60</th>\n",
       "      <th>rolling_mean_tmp_7_7</th>\n",
       "      <th>rolling_mean_tmp_7_14</th>\n",
       "      <th>rolling_mean_tmp_7_30</th>\n",
       "      <th>rolling_mean_tmp_7_60</th>\n",
       "      <th>rolling_mean_tmp_14_7</th>\n",
       "      <th>rolling_mean_tmp_14_14</th>\n",
       "      <th>rolling_mean_tmp_14_30</th>\n",
       "      <th>rolling_mean_tmp_14_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_012_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  d  sales  sales_lag_28  sales_lag_29  \\\n",
       "0  HOBBIES_1_008_CA_1_evaluation  1   12.0           NaN           NaN   \n",
       "1  HOBBIES_1_009_CA_1_evaluation  1    2.0           NaN           NaN   \n",
       "2  HOBBIES_1_010_CA_1_evaluation  1    0.0           NaN           NaN   \n",
       "3  HOBBIES_1_012_CA_1_evaluation  1    0.0           NaN           NaN   \n",
       "4  HOBBIES_1_015_CA_1_evaluation  1    4.0           NaN           NaN   \n",
       "\n",
       "   sales_lag_30  sales_lag_31  sales_lag_32  sales_lag_33  sales_lag_34  ...  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "1           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "2           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "3           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "4           NaN           NaN           NaN           NaN           NaN  ...   \n",
       "\n",
       "   rolling_mean_tmp_1_30  rolling_mean_tmp_1_60  rolling_mean_tmp_7_7  \\\n",
       "0                    NaN                    NaN                   NaN   \n",
       "1                    NaN                    NaN                   NaN   \n",
       "2                    NaN                    NaN                   NaN   \n",
       "3                    NaN                    NaN                   NaN   \n",
       "4                    NaN                    NaN                   NaN   \n",
       "\n",
       "   rolling_mean_tmp_7_14  rolling_mean_tmp_7_30  rolling_mean_tmp_7_60  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   rolling_mean_tmp_14_7  rolling_mean_tmp_14_14  rolling_mean_tmp_14_30  \\\n",
       "0                    NaN                     NaN                     NaN   \n",
       "1                    NaN                     NaN                     NaN   \n",
       "2                    NaN                     NaN                     NaN   \n",
       "3                    NaN                     NaN                     NaN   \n",
       "4                    NaN                     NaN                     NaN   \n",
       "\n",
       "   rolling_mean_tmp_14_60  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf1=pd.read_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d</th>\n",
       "      <th>enc_state_id_mean</th>\n",
       "      <th>enc_state_id_std</th>\n",
       "      <th>enc_store_id_mean</th>\n",
       "      <th>enc_store_id_std</th>\n",
       "      <th>enc_cat_id_mean</th>\n",
       "      <th>enc_cat_id_std</th>\n",
       "      <th>enc_dept_id_mean</th>\n",
       "      <th>enc_dept_id_std</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_store_id_cat_id_mean</th>\n",
       "      <th>enc_store_id_cat_id_std</th>\n",
       "      <th>enc_store_id_dept_id_mean</th>\n",
       "      <th>enc_store_id_dept_id_std</th>\n",
       "      <th>enc_item_id_mean</th>\n",
       "      <th>enc_item_id_std</th>\n",
       "      <th>enc_item_id_state_id_mean</th>\n",
       "      <th>enc_item_id_state_id_std</th>\n",
       "      <th>enc_item_id_store_id_mean</th>\n",
       "      <th>enc_item_id_store_id_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_008_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>4.589844</td>\n",
       "      <td>1.636719</td>\n",
       "      <td>4.460938</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>2.255859</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>2.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>3.121094</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>3.541016</td>\n",
       "      <td>4.687500</td>\n",
       "      <td>7.160156</td>\n",
       "      <td>6.582031</td>\n",
       "      <td>8.765625</td>\n",
       "      <td>7.230469</td>\n",
       "      <td>9.117188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_009_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>4.589844</td>\n",
       "      <td>1.636719</td>\n",
       "      <td>4.460938</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>2.255859</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>2.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>3.121094</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>3.541016</td>\n",
       "      <td>0.850098</td>\n",
       "      <td>1.752930</td>\n",
       "      <td>1.135742</td>\n",
       "      <td>2.099609</td>\n",
       "      <td>1.186523</td>\n",
       "      <td>2.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_010_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>4.589844</td>\n",
       "      <td>1.636719</td>\n",
       "      <td>4.460938</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>2.255859</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>2.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>3.121094</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>3.541016</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.861816</td>\n",
       "      <td>0.562012</td>\n",
       "      <td>0.827148</td>\n",
       "      <td>0.719238</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_012_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>4.589844</td>\n",
       "      <td>1.636719</td>\n",
       "      <td>4.460938</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>2.255859</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>2.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>3.121094</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>3.541016</td>\n",
       "      <td>0.383057</td>\n",
       "      <td>0.690430</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>0.726074</td>\n",
       "      <td>0.394043</td>\n",
       "      <td>0.649414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_015_CA_1_evaluation</td>\n",
       "      <td>1</td>\n",
       "      <td>1.574219</td>\n",
       "      <td>4.589844</td>\n",
       "      <td>1.636719</td>\n",
       "      <td>4.460938</td>\n",
       "      <td>0.708496</td>\n",
       "      <td>2.255859</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>2.541016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>3.121094</td>\n",
       "      <td>1.260742</td>\n",
       "      <td>3.541016</td>\n",
       "      <td>4.425781</td>\n",
       "      <td>6.679688</td>\n",
       "      <td>6.921875</td>\n",
       "      <td>8.359375</td>\n",
       "      <td>6.066406</td>\n",
       "      <td>7.351562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  d  enc_state_id_mean  enc_state_id_std  \\\n",
       "0  HOBBIES_1_008_CA_1_evaluation  1           1.574219          4.589844   \n",
       "1  HOBBIES_1_009_CA_1_evaluation  1           1.574219          4.589844   \n",
       "2  HOBBIES_1_010_CA_1_evaluation  1           1.574219          4.589844   \n",
       "3  HOBBIES_1_012_CA_1_evaluation  1           1.574219          4.589844   \n",
       "4  HOBBIES_1_015_CA_1_evaluation  1           1.574219          4.589844   \n",
       "\n",
       "   enc_store_id_mean  enc_store_id_std  enc_cat_id_mean  enc_cat_id_std  \\\n",
       "0           1.636719          4.460938         0.708496        2.255859   \n",
       "1           1.636719          4.460938         0.708496        2.255859   \n",
       "2           1.636719          4.460938         0.708496        2.255859   \n",
       "3           1.636719          4.460938         0.708496        2.255859   \n",
       "4           1.636719          4.460938         0.708496        2.255859   \n",
       "\n",
       "   enc_dept_id_mean  enc_dept_id_std  ...  enc_store_id_cat_id_mean  \\\n",
       "0          0.865234         2.541016  ...                  1.003906   \n",
       "1          0.865234         2.541016  ...                  1.003906   \n",
       "2          0.865234         2.541016  ...                  1.003906   \n",
       "3          0.865234         2.541016  ...                  1.003906   \n",
       "4          0.865234         2.541016  ...                  1.003906   \n",
       "\n",
       "   enc_store_id_cat_id_std  enc_store_id_dept_id_mean  \\\n",
       "0                 3.121094                   1.260742   \n",
       "1                 3.121094                   1.260742   \n",
       "2                 3.121094                   1.260742   \n",
       "3                 3.121094                   1.260742   \n",
       "4                 3.121094                   1.260742   \n",
       "\n",
       "   enc_store_id_dept_id_std  enc_item_id_mean  enc_item_id_std  \\\n",
       "0                  3.541016          4.687500         7.160156   \n",
       "1                  3.541016          0.850098         1.752930   \n",
       "2                  3.541016          0.610840         0.861816   \n",
       "3                  3.541016          0.383057         0.690430   \n",
       "4                  3.541016          4.425781         6.679688   \n",
       "\n",
       "   enc_item_id_state_id_mean  enc_item_id_state_id_std  \\\n",
       "0                   6.582031                  8.765625   \n",
       "1                   1.135742                  2.099609   \n",
       "2                   0.562012                  0.827148   \n",
       "3                   0.425781                  0.726074   \n",
       "4                   6.921875                  8.359375   \n",
       "\n",
       "   enc_item_id_store_id_mean  enc_item_id_store_id_std  \n",
       "0                   7.230469                  9.117188  \n",
       "1                   1.186523                  2.015625  \n",
       "2                   0.719238                  0.921875  \n",
       "3                   0.394043                  0.649414  \n",
       "4                   6.066406                  7.351562  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47735397, 40)\n",
      "(47735397, 24)\n"
     ]
    }
   ],
   "source": [
    "print(ddf.shape)\n",
    "print(ddf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
